{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into \"https://api.rhods-weka.bj30.p1.openshiftapps.com:6443\" as \"heyselbi\" using the token provided.\n",
      "\n",
      "You have access to 110 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"distributed-tf\".\n"
     ]
    }
   ],
   "source": [
    "!oc login --token=sha256~KiSeXt6HnPQoQX_af6KMcyGBpCPveQWp8iuNuce0e1o --server=https://api.rhods-weka.bj30.p1.openshiftapps.com:6443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mnist-tfjob.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnist-tfjob.yaml\n",
    "apiVersion: kubeflow.org/v1\n",
    "kind: TFJob\n",
    "metadata:\n",
    "  generateName: tfjob-multi-\n",
    "  namespace: distributed-tf\n",
    "spec:\n",
    "  cleanPodPolicy: Running\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      selector:\n",
    "        matchLabels:\n",
    "           app: distributed\n",
    "      replicas: 2\n",
    "      restartPolicy: Never\n",
    "      template:\n",
    "        metadata:\n",
    "          labels:\n",
    "            app: distributed\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:            \n",
    "          containers:\n",
    "          - name: tensorflow\n",
    "            image: \"quay.io/selbi/tf-mnist:ubi8-tf2.4.0\"\n",
    "            command: [\"python3\", \"/home/mnist/src/main.py\"] \n",
    "            imagePullPolicy: Always\n",
    "            env:\n",
    "              - name: LD_LIBRARY_PATH\n",
    "                value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/nccl/lib:/usr/local/cuda/lib:/usr/local/cuda-10.1/compat\n",
    "              - name: TF_FORCE_GPU_ALLOW_GROWTH\n",
    "                value: \"true\"\n",
    "          imagePullSecrets:\n",
    "            - name: \"selbi-build-pull-secret\"\n",
    "          nodeSelector:\n",
    "            node.kubernetes.io/instance-type: \"g4dn.12xlarge\"\n",
    "          affinity:\n",
    "            podAntiAffinity:\n",
    "               requiredDuringSchedulingIgnoredDuringExecution:\n",
    "               - labelSelector:\n",
    "                   matchExpressions:\n",
    "                   - key: app\n",
    "                     operator: In\n",
    "                     values:\n",
    "                     - distributed\n",
    "                 topologyKey: \"kubernetes.io/hostname\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfjob.kubeflow.org/tfjob-multi-4cq8w created\r\n"
     ]
    }
   ],
   "source": [
    "!oc create -f mnist-tfjob.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfjob-multi-4cq8w-worker-0     1/1     Running   0          4s\r\n",
      "tfjob-multi-4cq8w-worker-1     1/1     Running   0          4s\r\n"
     ]
    }
   ],
   "source": [
    "!oc get pods | grep tfjob-multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-16 14:50:24.329209: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> tfjob-multi-4cq8w-worker-0.distributed-tf.svc:2222, 1 -> tfjob-multi-4cq8w-worker-1.distributed-tf.svc:2222}\r\n"
     ]
    }
   ],
   "source": [
    "!oc logs pod/tfjob-multi-4cq8w-worker-0 | grep \"GrpcChannelCache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-16 14:50:24.283930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 13968 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1b.0, compute capability: 7.5)\r\n",
      "2021-07-16 14:50:24.284996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 13968 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:1c.0, compute capability: 7.5)\r\n",
      "2021-07-16 14:50:24.286042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 13968 MB memory) -> physical GPU (device: 2, name: Tesla T4, pci bus id: 0000:00:1d.0, compute capability: 7.5)\r\n",
      "2021-07-16 14:50:24.287090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 13968 MB memory) -> physical GPU (device: 3, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)\r\n"
     ]
    }
   ],
   "source": [
    "!oc logs pod/tfjob-multi-4cq8w-worker-0 | grep \"/job:worker/replica:0/task:0/device:GPU:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 16 14:50:47 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   35C    P0    28W /  70W |   1002MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   35C    P0    28W /  70W |    888MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   34C    P0    28W /  70W |    992MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   35C    P0    30W /  70W |    888MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1150358      C   python3                           999MiB |\n",
      "|    1   N/A  N/A   1150358      C   python3                           885MiB |\n",
      "|    2   N/A  N/A   1150358      C   python3                           989MiB |\n",
      "|    3   N/A  N/A   1150358      C   python3                           885MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!oc exec -it nvidia-driver-daemonset-268p9 nvidia-smi -n gpu-operator-resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 16 14:50:43 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   41C    P0    29W /  70W |    992MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   40C    P0    28W /  70W |    888MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   42C    P0    29W /  70W |    888MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   40C    P0    28W /  70W |   1002MiB / 15109MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1366917      C   python3                           989MiB |\n",
      "|    1   N/A  N/A   1366917      C   python3                           885MiB |\n",
      "|    2   N/A  N/A   1366917      C   python3                           885MiB |\n",
      "|    3   N/A  N/A   1366917      C   python3                           999MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!oc exec -it nvidia-driver-daemonset-vmxck nvidia-smi -n gpu-operator-resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
